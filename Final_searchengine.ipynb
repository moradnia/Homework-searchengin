{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# urls\n",
    "https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies3.html\n",
    "\n",
    "\n",
    "https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies2.html\n",
    "    \n",
    "https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [' https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies3.html',\n",
    "        'https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies2.html',\n",
    "        'https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies1.html']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from functools import reduce\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "from lxml import etree\n",
    "import lxml.html\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract movie links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_links = []\n",
    "for url in urls:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        for link in soup.find_all('a',href=True):\n",
    "            movie_links.append(link['href'])\n",
    "    except Exception as ex:\n",
    "        print('error is ', ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org/wiki/Z.P.G.',\n",
       " 'https://en.wikipedia.org/wiki/Zee_and_Co.',\n",
       " 'https://en.wikipedia.org/wiki/And_Now_the_Screaming_Starts!',\n",
       " 'https://en.wikipedia.org/wiki/The_Asphyx',\n",
       " 'https://en.wikipedia.org/wiki/Assassin_(1973_film)']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_links[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_sw_intro = []\n",
    "def rm_stopwords(text):\n",
    "    new_text = text.lower()\n",
    "    global tokens_without_sw_intro\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    text_tokens_intro = word_tokenize(new_text)\n",
    "    tokens_without_sw_intro = [word for word in text_tokens_intro if not word in stopwords.words()]\n",
    "   \n",
    "    return tokens_without_sw_intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words_intro =[]\n",
    "def rm_punctuation ():\n",
    "    global new_words_intro\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    punc_intro = \",\".join(tokens_without_sw_intro)\n",
    "    new_words_intro = tokenizer.tokenize(punc_intro)\n",
    "    return new_words_intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming_intro = []\n",
    "def stemming():\n",
    "    global streaming_intro\n",
    "    streaming_intro = []\n",
    "    ps = PorterStemmer()\n",
    "    for x in new_words_intro:\n",
    "        \n",
    "        streaming_intro.append(ps.stem(x))\n",
    "    return streaming_intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_intro = []\n",
    "def Final_normalize():\n",
    "    global streaming_intro\n",
    "    final_intro = []\n",
    "    for i in streaming_intro:\n",
    "        if len(i) > 3:\n",
    "            final_intro.append(i)\n",
    "    return \" \".join(final_intro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract title,intro,plot,table_info and save as tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_list = ['Directed by','Produced by','Written by','Starring','Music by','Release date','Running time','Countries',\n",
    "              'Language','Budget','Country']\n",
    "vocabulary = []\n",
    "\n",
    "for number,t in enumerate(movie_links[:5]):  \n",
    "    web_response = requests.get(t)\n",
    "    soup = BeautifulSoup(web_response.text, 'html.parser')\n",
    "    title = soup.title.text.split('-')\n",
    "    title = title[0]\n",
    "\n",
    "    # start intro\n",
    "    response = requests.get(t) \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    intro =  soup.find('div' , { \"id\" : \"toc\" }).find_all_previous('p')[::-1]\n",
    "    l=[]\n",
    "    for i in intro:\n",
    "        x = i.text.strip()\n",
    "        if x != '':\n",
    "            l.append(x)\n",
    "    y= \" \".join(l)\n",
    "    # remove stopwords\n",
    "    rm_stopword_intro = rm_stopwords(y)\n",
    "    #remove punchtuation\n",
    "    rm_punc_intro=rm_punctuation()\n",
    "    # stemmer\n",
    "    steming_intro = stemming()\n",
    "    # final_normalize\n",
    "    final_norm_intro = Final_normalize()\n",
    "    # vocabulary\n",
    "    vocab_intro = list(set(final_norm_intro.split()))\n",
    "    vocabulary.extend(vocab_intro)\n",
    "\n",
    "\n",
    "    # start plot\n",
    "    plot =  soup.find('span' , { \"class\" : \"mw-editsection-bracket\" }).find_next('p')\n",
    "    final_plot = plot.text.strip()\n",
    "    # remove stopwords\n",
    "    rm_stopword = rm_stopwords(final_plot)\n",
    "\n",
    "    # remove punch\n",
    "    rm_punc=rm_punctuation()\n",
    "\n",
    "    # stemmer\n",
    "    steming = stemming()\n",
    "\n",
    "    #final_normalize\n",
    "    final_norm_plot = Final_normalize()\n",
    "    vocab_plot = list(set(final_norm_plot.split()))\n",
    "    vocabulary.extend(vocab_plot)\n",
    "\n",
    "    #table\n",
    "    table = soup.find('table', class_ = 'infobox vevent')\n",
    "    d={}\n",
    "    for x in table.find_all('tr')[2:]:\n",
    "        if x.th.text.lower() in [l.lower() for l in check_list]:\n",
    "            for i in x.find_all('th'):\n",
    "                for y in x.find_all('td'):\n",
    "                    d[i.text]=y.text.lower()\n",
    "                    for i in check_list:\n",
    "                        if i not in d.keys():\n",
    "                            d[i]= 'NA'\n",
    "\n",
    "    if d['Countries'] == 'NA' and d['Country'] == 'NA':\n",
    "        del d['Country']\n",
    "    if d['Countries'] !='NA' and d['Country'] == 'NA':\n",
    "        del d['Country']\n",
    "    if d['Countries'] =='NA' and d['Country'] != 'NA':\n",
    "        del d['Countries']\n",
    "\n",
    "    w = d.values()\n",
    "    d_value = \"    \".join(list(w))\n",
    "    # start save to tsv\n",
    " \n",
    "    name = 'C:\\\\Users\\\\Profosor\\\\Desktop\\\\tsv\\\\article_{}.tsv'.format(number)\n",
    "    with open(name, 'wt', encoding='UTF-8') as out_file:\n",
    "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "        tsv_writer.writerow([title])\n",
    "        tsv_writer.writerow([final_norm_intro])\n",
    "        tsv_writer.writerow([final_norm_plot])\n",
    "        tsv_writer.writerow([t])\n",
    "        tsv_writer.writerow([d_value])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bomb', 'execut', 'direct', 'michael', 'growth', 'entir', 'world', 'featur', 'dystopian', 'whose', 'concern', 'sell', 'book', 'design', 'bleak', 'scienc', '1972', 'chaplin', 'popul', 'fiction', 'year', 'violat', 'oliv', 'best', 'american', 'earth', 'bound', 'danish', 'paul', 'denmark', 'oppress', 'reflect', 'futur', 'overpopul', 'govern', 'children', 'film', 'campu', 'inspir', 'ehrlich', 'almost', 'zero', 'star', 'geraldin', 'reed', 'short', 'perman', 'babyland', 'surfac', 'entir', 'world', 'visit', 'penalti', 'animatron', 'sever', 'bear', 'common', 'resourc', 'past', 'result', 'color', 'deterr', 'plastic', 'brainwash', 'dome', 'mask', 'coupl', 'need', 'pollut', 'conceiv', 'substitut', 'popul', 'breath', 'yearn', 'extinct', 'newborn', 'bright', 'smog', 'next', 'given', 'thick', 'citi', 'outsid', 'reduc', 'suffoc', 'well', 'earth', 'ultim', 'avail', 'affect', 'anim', 'household', 'cover', 'wear', 'futur', 'overpopul', 'becom', 'govern', 'children', 'life', 'death', 'size', 'decre', 'break', 'robot', 'contain', 'place', 'parent', 'dismal', 'settl', 'year', 'even', 'child', 'instead', 'peopl', 'tasteless', 'compani', 'york', 'direct', 'hutton', 'shot', 'shepperton', 'seven', 'drama', 'michael', 'base', 'last', 'woman', 'mullin', 'known', 'famili', 'separ', 'taylor', 'london', 'citat', 'edna', 'screenplay', 'whose', 'concern', 'album', 'cain', 'pictur', 'design', 'studio', '1972', 'singl', 'bicker', 'upon', 'coupl', 'need', 'locat', 'song', 'brien', 'fool', 'british', 'well', 'marriag', 'middl', 'novel', 'three', 'elizabeth', 'cover', 'film', 'circl', 'come', 'releas', 'columbia', 'night', 'brian', 'star', 'side', 'susannah', 'theme', 'director', 'york', 'rock', 'drawn', 'quiet', 'michael', 'architect', 'loud', 'term', 'match', 'taylor', 'whose', 'owner', 'cain', 'person', 'blake', 'socialit', 'coars', 'frequent', 'complet', 'antithesi', 'husband', 'spar', 'antic', 'marriag', 'sick', 'stella', 'elizabeth', 'boutiqu', 'robert', 'someth', 'susannah', 'verbal', 'compani', 'antholog', 'direct', 'fengriffen', 'length', 'portmanteau', 'base', 'known', 'featur', 'ogilvi', 'start', 'screenplay', 'scream', 'case', 'bride', 'roger', 'bray', '1970', 'ward', 'beacham', 'four', 'mage', 'cush', 'novella', 'hous', 'court', 'patrick', 'david', 'british', 'best', 'gothic', 'baker', 'villag', 'amicu', 'horror', 'herbert', 'written', 'film', 'larg', '1973', 'stephani', 'near', 'oakley', 'marshal', 'star', 'stori', 'hand', 'show', 'fengriffen', 'empti', 'live', 'fiancé', 'socket', 'lodg', 'circumst', 'tell', 'housemaid', 'famili', 'estat', 'vision', 'answer', 'ogilvi', 'birthmark', 'sever', 'sila', 'woodsman', 'ident', 'maitland', 'hack', 'chaperon', 'strangl', 'thrown', 'beacham', 'corps', 'bedroom', 'kill', 'move', 'undead', 'bizarr', 'vanish', 'experi', 'other', 'luke', 'hous', 'question', 'right', 'stair', 'solicitor', 'terrifi', 'later', 'charl', 'encount', 'catherin', 'disturb', 'begin', 'film', 'anyth', 'death', 'aunt', 'rape', 'night', 'reluct', 'willing', 'edith', '1795', 'evil', 'spirit', 'attack', 'heavili', 'british', 'fiction', 'film', 'direct', 'death', 'robert', 'powel', 'scienc', 'dead', '1972', 'asphyx', 'newbrook', 'known', 'star', 'spirit', 'stephen', 'horror', 'part', 'studi', 'societi', 'cunningham', 'phenomena', 'properli', 'strang', 'photo', 'moment', 'conclud', 'evid', 'hugo', 'result', 'though', 'england', 'psychic', 'latest', 'sceptic', 'individu', 'captur', 'philanthrop', 'bodi', 'around', 'soul', 'smudg', 'photograph', 'escap', 'hover', 'investig', 'done', 'depict', 'scientist', 'death', 'member', 'begun', 'parapsycholog', 'victorian', 'windsor', 'british', 'edward', 'direct', 'film', 'hendri', '1973', 'crane', 'frank', 'thriller', 'star', 'judd', 'assassin', 'british', 'govern', 'ministri', 'leak', 'hire', 'kill', 'offici', 'defenc', 'secret', 'suspect', 'assassin']\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# way true\n",
    "def createDictionary(directory):\n",
    "    wordsAdded = {}\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(directory)\n",
    "    fileList = os.listdir(directory)\n",
    "    for file in fileList:\n",
    "        with open(file, 'r') as f:\n",
    "            total_word = []\n",
    "            words = f.read().lower().strip().split('\\n')\n",
    "            for word in words[2].split():\n",
    "                for i,term in enumerate(vocabulary):\n",
    "                    if word == term:\n",
    "                        if word not in wordsAdded.keys():\n",
    "                            wordsAdded[word] = {}\n",
    "                            wordsAdded[word]['fileNames'] = []\n",
    "                            wordsAdded[word]['Term_id'] = []\n",
    "                            \n",
    "                        if f.name not in wordsAdded[word]['fileNames'] and i not in wordsAdded[word]['Term_id']:\n",
    "                            wordsAdded[word]['fileNames'] += [f.name]\n",
    "                            wordsAdded[word]['Term_id'] += [i]\n",
    "            for word in words[4].split():\n",
    "                    for i,term in enumerate(vocabulary):\n",
    "                        if word == term:\n",
    "                            if word not in wordsAdded.keys():\n",
    "                                wordsAdded[word] = {}\n",
    "                                wordsAdded[word]['fileNames'] = []\n",
    "                                wordsAdded[word]['Term_id'] = []\n",
    "\n",
    "                            if f.name not in wordsAdded[word]['fileNames'] and i not in wordsAdded[word]['Term_id']:\n",
    "                                wordsAdded[word]['fileNames'] += [f.name]\n",
    "                                wordsAdded[word]['Term_id'] += [i]\n",
    "\n",
    "\n",
    "    os.chdir(cwd)\n",
    "    return wordsAdded\n",
    "def writeToFile(words):\n",
    "    with open('C:\\\\Users\\\\Profosor\\\\Desktop\\\\tsv\\\\index-file.csv', 'w',encoding='UTF-8') as indexFile:\n",
    "\n",
    "        fieldNames = ['Term_id','word', 'fileNames']\n",
    "\n",
    "        csvWriter = csv.DictWriter(indexFile, fieldnames= fieldNames)\n",
    "\n",
    "        csvWriter.writeheader()\n",
    "\n",
    "        for word, fileDetails in words.items():\n",
    "\n",
    "            fileNameString = reduce(lambda x, y: x + \", \" + y, fileDetails['fileNames'])\n",
    "            fileTerm_id = reduce(lambda  x, y: x, fileDetails['Term_id'])\n",
    "            csvWriter.writerow({'word': word, 'fileNames': fileNameString, 'Term_id': fileTerm_id})\n",
    "\n",
    "\n",
    "def main():\n",
    "    directory = 'C:\\\\Users\\\\Profosor\\\\Desktop\\\\tsv'\n",
    "    writeToFile(createDictionary(directory))\n",
    "    print(\"Finished\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bomb', 'execut', 'direct', 'michael', 'growth', 'entir', 'world', 'featur', 'dystopian', 'whose', 'concern', 'sell', 'book', 'design', 'bleak', 'scienc', '1972', 'chaplin', 'popul', 'fiction', 'year', 'violat', 'oliv', 'best', 'american', 'earth', 'bound', 'danish', 'paul', 'denmark', 'oppress', 'reflect', 'futur', 'overpopul', 'govern', 'children', 'film', 'campu', 'inspir', 'ehrlich', 'almost', 'zero', 'star', 'geraldin', 'reed', 'short', 'perman', 'babyland', 'surfac', 'entir', 'world', 'visit', 'penalti', 'animatron', 'sever', 'bear', 'common', 'resourc', 'past', 'result', 'color', 'deterr', 'plastic', 'brainwash', 'dome', 'mask', 'coupl', 'need', 'pollut', 'conceiv', 'substitut', 'popul', 'breath', 'yearn', 'extinct', 'newborn', 'bright', 'smog', 'next', 'given', 'thick', 'citi', 'outsid', 'reduc', 'suffoc', 'well', 'earth', 'ultim', 'avail', 'affect', 'anim', 'household', 'cover', 'wear', 'futur', 'overpopul', 'becom', 'govern', 'children', 'life', 'death', 'size', 'decre', 'break', 'robot', 'contain', 'place', 'parent', 'dismal', 'settl', 'year', 'even', 'child', 'instead', 'peopl', 'tasteless', 'compani', 'york', 'direct', 'hutton', 'shot', 'shepperton', 'seven', 'drama', 'michael', 'base', 'last', 'woman', 'mullin', 'known', 'famili', 'separ', 'taylor', 'london', 'citat', 'edna', 'screenplay', 'whose', 'concern', 'album', 'cain', 'pictur', 'design', 'studio', '1972', 'singl', 'bicker', 'upon', 'coupl', 'need', 'locat', 'song', 'brien', 'fool', 'british', 'well', 'marriag', 'middl', 'novel', 'three', 'elizabeth', 'cover', 'film', 'circl', 'come', 'releas', 'columbia', 'night', 'brian', 'star', 'side', 'susannah', 'theme', 'director', 'york', 'rock', 'drawn', 'quiet', 'michael', 'architect', 'loud', 'term', 'match', 'taylor', 'whose', 'owner', 'cain', 'person', 'blake', 'socialit', 'coars', 'frequent', 'complet', 'antithesi', 'husband', 'spar', 'antic', 'marriag', 'sick', 'stella', 'elizabeth', 'boutiqu', 'robert', 'someth', 'susannah', 'verbal', 'compani', 'antholog', 'direct', 'fengriffen', 'length', 'portmanteau', 'base', 'known', 'featur', 'ogilvi', 'start', 'screenplay', 'scream', 'case', 'bride', 'roger', 'bray', '1970', 'ward', 'beacham', 'four', 'mage', 'cush', 'novella', 'hous', 'court', 'patrick', 'david', 'british', 'best', 'gothic', 'baker', 'villag', 'amicu', 'horror', 'herbert', 'written', 'film', 'larg', '1973', 'stephani', 'near', 'oakley', 'marshal', 'star', 'stori', 'hand', 'show', 'fengriffen', 'empti', 'live', 'fiancé', 'socket', 'lodg', 'circumst', 'tell', 'housemaid', 'famili', 'estat', 'vision', 'answer', 'ogilvi', 'birthmark', 'sever', 'sila', 'woodsman', 'ident', 'maitland', 'hack', 'chaperon', 'strangl', 'thrown', 'beacham', 'corps', 'bedroom', 'kill', 'move', 'undead', 'bizarr', 'vanish', 'experi', 'other', 'luke', 'hous', 'question', 'right', 'stair', 'solicitor', 'terrifi', 'later', 'charl', 'encount', 'catherin', 'disturb', 'begin', 'film', 'anyth', 'death', 'aunt', 'rape', 'night', 'reluct', 'willing', 'edith', '1795', 'evil', 'spirit', 'attack', 'heavili', 'british', 'fiction', 'film', 'direct', 'death', 'robert', 'powel', 'scienc', 'dead', '1972', 'asphyx', 'newbrook', 'known', 'star', 'spirit', 'stephen', 'horror', 'part', 'studi', 'societi', 'cunningham', 'phenomena', 'properli', 'strang', 'photo', 'moment', 'conclud', 'evid', 'hugo', 'result', 'though', 'england', 'psychic', 'latest', 'sceptic', 'individu', 'captur', 'philanthrop', 'bodi', 'around', 'soul', 'smudg', 'photograph', 'escap', 'hover', 'investig', 'done', 'depict', 'scientist', 'death', 'member', 'begun', 'parapsycholog', 'victorian', 'windsor', 'british', 'edward', 'direct', 'film', 'hendri', '1973', 'crane', 'frank', 'thriller', 'star', 'judd', 'assassin', 'british', 'govern', 'ministri', 'leak', 'hire', 'kill', 'offici', 'defenc', 'secret', 'suspect', 'assassin']\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'short': {'fileNames': ['article_0.tsv'], 'Term_id': [45]},\n",
       " 'zero': {'fileNames': ['article_0.tsv'], 'Term_id': [41]},\n",
       " 'popul': {'fileNames': ['article_0.tsv'], 'Term_id': [18]},\n",
       " 'growth': {'fileNames': ['article_0.tsv'], 'Term_id': [4]},\n",
       " '1972': {'fileNames': ['article_0.tsv', 'article_1.tsv', 'article_3.tsv'],\n",
       "  'Term_id': [16, 144, 324]},\n",
       " 'danish': {'fileNames': ['article_0.tsv'], 'Term_id': [27]},\n",
       " 'american': {'fileNames': ['article_0.tsv'], 'Term_id': [24]},\n",
       " 'dystopian': {'fileNames': ['article_0.tsv'], 'Term_id': [8]},\n",
       " 'scienc': {'fileNames': ['article_0.tsv', 'article_3.tsv'],\n",
       "  'Term_id': [15, 322]},\n",
       " 'fiction': {'fileNames': ['article_0.tsv', 'article_3.tsv'],\n",
       "  'Term_id': [19, 316]},\n",
       " 'film': {'fileNames': ['article_0.tsv',\n",
       "   'article_1.tsv',\n",
       "   'article_2.tsv',\n",
       "   'article_3.tsv',\n",
       "   'article_4.tsv'],\n",
       "  'Term_id': [36, 162, 243, 301, 317]},\n",
       " 'direct': {'fileNames': ['article_0.tsv',\n",
       "   'article_1.tsv',\n",
       "   'article_2.tsv',\n",
       "   'article_3.tsv',\n",
       "   'article_4.tsv'],\n",
       "  'Term_id': [2, 118, 208, 318, 372]},\n",
       " 'michael': {'fileNames': ['article_0.tsv', 'article_1.tsv'],\n",
       "  'Term_id': [3, 124]},\n",
       " 'campu': {'fileNames': ['article_0.tsv'], 'Term_id': [37]},\n",
       " 'star': {'fileNames': ['article_0.tsv',\n",
       "   'article_1.tsv',\n",
       "   'article_2.tsv',\n",
       "   'article_3.tsv',\n",
       "   'article_4.tsv'],\n",
       "  'Term_id': [42, 169, 250, 328, 379]},\n",
       " 'oliv': {'fileNames': ['article_0.tsv'], 'Term_id': [22]},\n",
       " 'reed': {'fileNames': ['article_0.tsv'], 'Term_id': [44]},\n",
       " 'geraldin': {'fileNames': ['article_0.tsv'], 'Term_id': [43]},\n",
       " 'chaplin': {'fileNames': ['article_0.tsv'], 'Term_id': [17]},\n",
       " 'inspir': {'fileNames': ['article_0.tsv'], 'Term_id': [38]},\n",
       " 'best': {'fileNames': ['article_0.tsv', 'article_2.tsv'],\n",
       "  'Term_id': [23, 235]},\n",
       " 'sell': {'fileNames': ['article_0.tsv'], 'Term_id': [11]},\n",
       " 'book': {'fileNames': ['article_0.tsv'], 'Term_id': [12]},\n",
       " 'bomb': {'fileNames': ['article_0.tsv'], 'Term_id': [0]},\n",
       " 'paul': {'fileNames': ['article_0.tsv'], 'Term_id': [28]},\n",
       " 'ehrlich': {'fileNames': ['article_0.tsv'], 'Term_id': [39]},\n",
       " 'concern': {'fileNames': ['article_0.tsv', 'article_1.tsv'],\n",
       "  'Term_id': [10, 138]},\n",
       " 'overpopul': {'fileNames': ['article_0.tsv'], 'Term_id': [33]},\n",
       " 'futur': {'fileNames': ['article_0.tsv'], 'Term_id': [32]},\n",
       " 'earth': {'fileNames': ['article_0.tsv'], 'Term_id': [25]},\n",
       " 'whose': {'fileNames': ['article_0.tsv', 'article_1.tsv'],\n",
       "  'Term_id': [9, 137]},\n",
       " 'world': {'fileNames': ['article_0.tsv'], 'Term_id': [6]},\n",
       " 'govern': {'fileNames': ['article_0.tsv', 'article_4.tsv'],\n",
       "  'Term_id': [34, 97]},\n",
       " 'execut': {'fileNames': ['article_0.tsv'], 'Term_id': [1]},\n",
       " 'violat': {'fileNames': ['article_0.tsv'], 'Term_id': [21]},\n",
       " 'year': {'fileNames': ['article_0.tsv'], 'Term_id': [20]},\n",
       " 'children': {'fileNames': ['article_0.tsv'], 'Term_id': [35]},\n",
       " 'denmark': {'fileNames': ['article_0.tsv'], 'Term_id': [29]},\n",
       " 'almost': {'fileNames': ['article_0.tsv'], 'Term_id': [40]},\n",
       " 'entir': {'fileNames': ['article_0.tsv'], 'Term_id': [5]},\n",
       " 'bound': {'fileNames': ['article_0.tsv'], 'Term_id': [26]},\n",
       " 'featur': {'fileNames': ['article_0.tsv', 'article_2.tsv'],\n",
       "  'Term_id': [7, 214]},\n",
       " 'design': {'fileNames': ['article_0.tsv', 'article_1.tsv'],\n",
       "  'Term_id': [13, 142]},\n",
       " 'reflect': {'fileNames': ['article_0.tsv'], 'Term_id': [31]},\n",
       " 'bleak': {'fileNames': ['article_0.tsv'], 'Term_id': [14]},\n",
       " 'oppress': {'fileNames': ['article_0.tsv'], 'Term_id': [30]},\n",
       " 'becom': {'fileNames': ['article_0.tsv'], 'Term_id': [96]},\n",
       " 'sever': {'fileNames': ['article_0.tsv', 'article_2.tsv'],\n",
       "  'Term_id': [54, 269]},\n",
       " 'pollut': {'fileNames': ['article_0.tsv'], 'Term_id': [68]},\n",
       " 'peopl': {'fileNames': ['article_0.tsv'], 'Term_id': [114]},\n",
       " 'need': {'fileNames': ['article_0.tsv', 'article_1.tsv'],\n",
       "  'Term_id': [67, 149]},\n",
       " 'wear': {'fileNames': ['article_0.tsv'], 'Term_id': [93]},\n",
       " 'breath': {'fileNames': ['article_0.tsv'], 'Term_id': [72]},\n",
       " 'mask': {'fileNames': ['article_0.tsv'], 'Term_id': [65]},\n",
       " 'outsid': {'fileNames': ['article_0.tsv'], 'Term_id': [82]},\n",
       " 'affect': {'fileNames': ['article_0.tsv'], 'Term_id': [89]},\n",
       " 'avail': {'fileNames': ['article_0.tsv'], 'Term_id': [88]},\n",
       " 'resourc': {'fileNames': ['article_0.tsv'], 'Term_id': [57]},\n",
       " 'perman': {'fileNames': ['article_0.tsv'], 'Term_id': [46]},\n",
       " 'thick': {'fileNames': ['article_0.tsv'], 'Term_id': [80]},\n",
       " 'smog': {'fileNames': ['article_0.tsv'], 'Term_id': [77]},\n",
       " 'settl': {'fileNames': ['article_0.tsv'], 'Term_id': [109]},\n",
       " 'dismal': {'fileNames': ['article_0.tsv'], 'Term_id': [108]},\n",
       " 'citi': {'fileNames': ['article_0.tsv'], 'Term_id': [81]},\n",
       " 'cover': {'fileNames': ['article_0.tsv', 'article_1.tsv'],\n",
       "  'Term_id': [92, 161]},\n",
       " 'surfac': {'fileNames': ['article_0.tsv'], 'Term_id': [48]},\n",
       " 'anim': {'fileNames': ['article_0.tsv'], 'Term_id': [90]},\n",
       " 'even': {'fileNames': ['article_0.tsv'], 'Term_id': [111]},\n",
       " 'common': {'fileNames': ['article_0.tsv'], 'Term_id': [56]},\n",
       " 'household': {'fileNames': ['article_0.tsv'], 'Term_id': [91]},\n",
       " 'extinct': {'fileNames': ['article_0.tsv'], 'Term_id': [74]},\n",
       " 'tasteless': {'fileNames': ['article_0.tsv'], 'Term_id': [115]},\n",
       " 'bright': {'fileNames': ['article_0.tsv'], 'Term_id': [76]},\n",
       " 'color': {'fileNames': ['article_0.tsv'], 'Term_id': [60]},\n",
       " 'past': {'fileNames': ['article_0.tsv'], 'Term_id': [58]},\n",
       " 'plastic': {'fileNames': ['article_0.tsv'], 'Term_id': [62]},\n",
       " 'contain': {'fileNames': ['article_0.tsv'], 'Term_id': [105]},\n",
       " 'reduc': {'fileNames': ['article_0.tsv'], 'Term_id': [83]},\n",
       " 'decre': {'fileNames': ['article_0.tsv'], 'Term_id': [102]},\n",
       " 'conceiv': {'fileNames': ['article_0.tsv'], 'Term_id': [69]},\n",
       " 'next': {'fileNames': ['article_0.tsv'], 'Term_id': [78]},\n",
       " 'break': {'fileNames': ['article_0.tsv'], 'Term_id': [103]},\n",
       " 'result': {'fileNames': ['article_0.tsv', 'article_3.tsv'],\n",
       "  'Term_id': [59, 344]},\n",
       " 'death': {'fileNames': ['article_0.tsv', 'article_2.tsv', 'article_3.tsv'],\n",
       "  'Term_id': [100, 303, 319]},\n",
       " 'penalti': {'fileNames': ['article_0.tsv'], 'Term_id': [52]},\n",
       " 'parent': {'fileNames': ['article_0.tsv'], 'Term_id': [107]},\n",
       " 'well': {'fileNames': ['article_0.tsv', 'article_1.tsv'],\n",
       "  'Term_id': [85, 155]},\n",
       " 'newborn': {'fileNames': ['article_0.tsv'], 'Term_id': [75]},\n",
       " 'brainwash': {'fileNames': ['article_0.tsv'], 'Term_id': [63]},\n",
       " 'robot': {'fileNames': ['article_0.tsv'], 'Term_id': [104]},\n",
       " 'substitut': {'fileNames': ['article_0.tsv'], 'Term_id': [70]},\n",
       " 'yearn': {'fileNames': ['article_0.tsv'], 'Term_id': [73]},\n",
       " 'ultim': {'fileNames': ['article_0.tsv'], 'Term_id': [87]},\n",
       " 'deterr': {'fileNames': ['article_0.tsv'], 'Term_id': [61]},\n",
       " 'place': {'fileNames': ['article_0.tsv'], 'Term_id': [106]},\n",
       " 'dome': {'fileNames': ['article_0.tsv'], 'Term_id': [64]},\n",
       " 'suffoc': {'fileNames': ['article_0.tsv'], 'Term_id': [84]},\n",
       " 'coupl': {'fileNames': ['article_0.tsv', 'article_1.tsv'],\n",
       "  'Term_id': [66, 148]},\n",
       " 'child': {'fileNames': ['article_0.tsv'], 'Term_id': [112]},\n",
       " 'bear': {'fileNames': ['article_0.tsv'], 'Term_id': [55]},\n",
       " 'visit': {'fileNames': ['article_0.tsv'], 'Term_id': [51]},\n",
       " 'babyland': {'fileNames': ['article_0.tsv'], 'Term_id': [47]},\n",
       " 'given': {'fileNames': ['article_0.tsv'], 'Term_id': [79]},\n",
       " 'life': {'fileNames': ['article_0.tsv'], 'Term_id': [99]},\n",
       " 'size': {'fileNames': ['article_0.tsv'], 'Term_id': [101]},\n",
       " 'animatron': {'fileNames': ['article_0.tsv'], 'Term_id': [53]},\n",
       " 'instead': {'fileNames': ['article_0.tsv'], 'Term_id': [113]},\n",
       " 'known': {'fileNames': ['article_1.tsv', 'article_2.tsv', 'article_3.tsv'],\n",
       "  'Term_id': [129, 213, 327]},\n",
       " 'compani': {'fileNames': ['article_1.tsv', 'article_2.tsv'],\n",
       "  'Term_id': [116, 206]},\n",
       " 'british': {'fileNames': ['article_1.tsv',\n",
       "   'article_2.tsv',\n",
       "   'article_3.tsv',\n",
       "   'article_4.tsv'],\n",
       "  'Term_id': [154, 234, 315, 370]},\n",
       " 'drama': {'fileNames': ['article_1.tsv'], 'Term_id': [123]},\n",
       " 'brian': {'fileNames': ['article_1.tsv'], 'Term_id': [168]},\n",
       " 'hutton': {'fileNames': ['article_1.tsv'], 'Term_id': [119]},\n",
       " 'elizabeth': {'fileNames': ['article_1.tsv'], 'Term_id': [160]},\n",
       " 'taylor': {'fileNames': ['article_1.tsv'], 'Term_id': [132]},\n",
       " 'cain': {'fileNames': ['article_1.tsv'], 'Term_id': [140]},\n",
       " 'susannah': {'fileNames': ['article_1.tsv'], 'Term_id': [171]},\n",
       " 'york': {'fileNames': ['article_1.tsv'], 'Term_id': [117]},\n",
       " 'releas': {'fileNames': ['article_1.tsv'], 'Term_id': [165]},\n",
       " 'columbia': {'fileNames': ['article_1.tsv'], 'Term_id': [166]},\n",
       " 'pictur': {'fileNames': ['article_1.tsv'], 'Term_id': [141]},\n",
       " 'base': {'fileNames': ['article_1.tsv', 'article_2.tsv'],\n",
       "  'Term_id': [125, 212]},\n",
       " 'upon': {'fileNames': ['article_1.tsv'], 'Term_id': [147]},\n",
       " 'novel': {'fileNames': ['article_1.tsv'], 'Term_id': [158]},\n",
       " 'edna': {'fileNames': ['article_1.tsv'], 'Term_id': [135]},\n",
       " 'brien': {'fileNames': ['article_1.tsv'], 'Term_id': [152]},\n",
       " 'screenplay': {'fileNames': ['article_1.tsv', 'article_2.tsv'],\n",
       "  'Term_id': [136, 217]},\n",
       " 'middl': {'fileNames': ['article_1.tsv'], 'Term_id': [157]},\n",
       " 'bicker': {'fileNames': ['article_1.tsv'], 'Term_id': [146]},\n",
       " 'marriag': {'fileNames': ['article_1.tsv'], 'Term_id': [156]},\n",
       " 'last': {'fileNames': ['article_1.tsv'], 'Term_id': [126]},\n",
       " 'woman': {'fileNames': ['article_1.tsv'], 'Term_id': [127]},\n",
       " 'come': {'fileNames': ['article_1.tsv'], 'Term_id': [164]},\n",
       " 'shot': {'fileNames': ['article_1.tsv'], 'Term_id': [120]},\n",
       " 'shepperton': {'fileNames': ['article_1.tsv'], 'Term_id': [121]},\n",
       " 'studio': {'fileNames': ['article_1.tsv'], 'Term_id': [143]},\n",
       " 'locat': {'fileNames': ['article_1.tsv'], 'Term_id': [150]},\n",
       " 'london': {'fileNames': ['article_1.tsv'], 'Term_id': [133]},\n",
       " 'director': {'fileNames': ['article_1.tsv'], 'Term_id': [173]},\n",
       " 'mullin': {'fileNames': ['article_1.tsv'], 'Term_id': [128]},\n",
       " 'theme': {'fileNames': ['article_1.tsv'], 'Term_id': [172]},\n",
       " 'song': {'fileNames': ['article_1.tsv'], 'Term_id': [151]},\n",
       " 'circl': {'fileNames': ['article_1.tsv'], 'Term_id': [163]},\n",
       " 'three': {'fileNames': ['article_1.tsv'], 'Term_id': [159]},\n",
       " 'night': {'fileNames': ['article_1.tsv', 'article_2.tsv'],\n",
       "  'Term_id': [167, 306]},\n",
       " 'album': {'fileNames': ['article_1.tsv'], 'Term_id': [139]},\n",
       " 'seven': {'fileNames': ['article_1.tsv'], 'Term_id': [122]},\n",
       " 'separ': {'fileNames': ['article_1.tsv'], 'Term_id': [131]},\n",
       " 'fool': {'fileNames': ['article_1.tsv'], 'Term_id': [153]},\n",
       " 'side': {'fileNames': ['article_1.tsv'], 'Term_id': [170]},\n",
       " 'singl': {'fileNames': ['article_1.tsv'], 'Term_id': [145]},\n",
       " 'famili': {'fileNames': ['article_1.tsv', 'article_2.tsv'],\n",
       "  'Term_id': [130, 263]},\n",
       " 'citat': {'fileNames': ['article_1.tsv'], 'Term_id': [134]},\n",
       " 'blake': {'fileNames': ['article_1.tsv'], 'Term_id': [188]},\n",
       " 'loud': {'fileNames': ['article_1.tsv'], 'Term_id': [180]},\n",
       " 'coars': {'fileNames': ['article_1.tsv'], 'Term_id': [190]},\n",
       " 'someth': {'fileNames': ['article_1.tsv'], 'Term_id': [203]},\n",
       " 'socialit': {'fileNames': ['article_1.tsv'], 'Term_id': [189]},\n",
       " 'architect': {'fileNames': ['article_1.tsv'], 'Term_id': [179]},\n",
       " 'husband': {'fileNames': ['article_1.tsv'], 'Term_id': [194]},\n",
       " 'robert': {'fileNames': ['article_1.tsv', 'article_3.tsv'],\n",
       "  'Term_id': [202, 320]},\n",
       " 'rock': {'fileNames': ['article_1.tsv'], 'Term_id': [175]},\n",
       " 'frequent': {'fileNames': ['article_1.tsv'], 'Term_id': [191]},\n",
       " 'verbal': {'fileNames': ['article_1.tsv'], 'Term_id': [205]},\n",
       " 'spar': {'fileNames': ['article_1.tsv'], 'Term_id': [195]},\n",
       " 'match': {'fileNames': ['article_1.tsv'], 'Term_id': [182]},\n",
       " 'sick': {'fileNames': ['article_1.tsv'], 'Term_id': [198]},\n",
       " 'antic': {'fileNames': ['article_1.tsv'], 'Term_id': [196]},\n",
       " 'drawn': {'fileNames': ['article_1.tsv'], 'Term_id': [176]},\n",
       " 'quiet': {'fileNames': ['article_1.tsv'], 'Term_id': [177]},\n",
       " 'boutiqu': {'fileNames': ['article_1.tsv'], 'Term_id': [201]},\n",
       " 'owner': {'fileNames': ['article_1.tsv'], 'Term_id': [185]},\n",
       " 'stella': {'fileNames': ['article_1.tsv'], 'Term_id': [199]},\n",
       " 'complet': {'fileNames': ['article_1.tsv'], 'Term_id': [192]},\n",
       " 'antithesi': {'fileNames': ['article_1.tsv'], 'Term_id': [193]},\n",
       " 'term': {'fileNames': ['article_1.tsv'], 'Term_id': [181]},\n",
       " 'person': {'fileNames': ['article_1.tsv'], 'Term_id': [187]},\n",
       " 'scream': {'fileNames': ['article_2.tsv'], 'Term_id': [218]},\n",
       " 'start': {'fileNames': ['article_2.tsv'], 'Term_id': [216]},\n",
       " '1973': {'fileNames': ['article_2.tsv', 'article_4.tsv'],\n",
       "  'Term_id': [245, 375]},\n",
       " 'gothic': {'fileNames': ['article_2.tsv'], 'Term_id': [236]},\n",
       " 'horror': {'fileNames': ['article_2.tsv', 'article_3.tsv'],\n",
       "  'Term_id': [240, 331]},\n",
       " 'ward': {'fileNames': ['article_2.tsv'], 'Term_id': [224]},\n",
       " 'baker': {'fileNames': ['article_2.tsv'], 'Term_id': [237]},\n",
       " 'cush': {'fileNames': ['article_2.tsv'], 'Term_id': [228]},\n",
       " 'herbert': {'fileNames': ['article_2.tsv'], 'Term_id': [241]},\n",
       " 'patrick': {'fileNames': ['article_2.tsv'], 'Term_id': [232]},\n",
       " 'mage': {'fileNames': ['article_2.tsv'], 'Term_id': [227]},\n",
       " 'stephani': {'fileNames': ['article_2.tsv'], 'Term_id': [246]},\n",
       " 'beacham': {'fileNames': ['article_2.tsv'], 'Term_id': [225]},\n",
       " 'ogilvi': {'fileNames': ['article_2.tsv'], 'Term_id': [215]},\n",
       " 'length': {'fileNames': ['article_2.tsv'], 'Term_id': [210]},\n",
       " 'stori': {'fileNames': ['article_2.tsv'], 'Term_id': [251]},\n",
       " 'amicu': {'fileNames': ['article_2.tsv'], 'Term_id': [239]},\n",
       " 'antholog': {'fileNames': ['article_2.tsv'], 'Term_id': [207]},\n",
       " 'portmanteau': {'fileNames': ['article_2.tsv'], 'Term_id': [211]},\n",
       " 'written': {'fileNames': ['article_2.tsv'], 'Term_id': [242]},\n",
       " 'roger': {'fileNames': ['article_2.tsv'], 'Term_id': [221]},\n",
       " 'marshal': {'fileNames': ['article_2.tsv'], 'Term_id': [249]},\n",
       " '1970': {'fileNames': ['article_2.tsv'], 'Term_id': [223]},\n",
       " 'novella': {'fileNames': ['article_2.tsv'], 'Term_id': [229]},\n",
       " 'fengriffen': {'fileNames': ['article_2.tsv'], 'Term_id': [209]},\n",
       " 'david': {'fileNames': ['article_2.tsv'], 'Term_id': [233]},\n",
       " 'case': {'fileNames': ['article_2.tsv'], 'Term_id': [219]},\n",
       " 'bride': {'fileNames': ['article_2.tsv'], 'Term_id': [220]},\n",
       " 'larg': {'fileNames': ['article_2.tsv'], 'Term_id': [244]},\n",
       " 'hous': {'fileNames': ['article_2.tsv'], 'Term_id': [230]},\n",
       " 'oakley': {'fileNames': ['article_2.tsv'], 'Term_id': [248]},\n",
       " 'court': {'fileNames': ['article_2.tsv'], 'Term_id': [231]},\n",
       " 'near': {'fileNames': ['article_2.tsv'], 'Term_id': [247]},\n",
       " 'bray': {'fileNames': ['article_2.tsv'], 'Term_id': [222]},\n",
       " 'villag': {'fileNames': ['article_2.tsv'], 'Term_id': [238]},\n",
       " 'four': {'fileNames': ['article_2.tsv'], 'Term_id': [226]},\n",
       " 'begin': {'fileNames': ['article_2.tsv'], 'Term_id': [300]},\n",
       " '1795': {'fileNames': ['article_2.tsv'], 'Term_id': [310]},\n",
       " 'move': {'fileNames': ['article_2.tsv'], 'Term_id': [282]},\n",
       " 'charl': {'fileNames': ['article_2.tsv'], 'Term_id': [296]},\n",
       " 'estat': {'fileNames': ['article_2.tsv'], 'Term_id': [264]},\n",
       " 'catherin': {'fileNames': ['article_2.tsv'], 'Term_id': [298]},\n",
       " 'experi': {'fileNames': ['article_2.tsv'], 'Term_id': [286]},\n",
       " 'terrifi': {'fileNames': ['article_2.tsv'], 'Term_id': [294]},\n",
       " 'vision': {'fileNames': ['article_2.tsv'], 'Term_id': [265]},\n",
       " 'undead': {'fileNames': ['article_2.tsv'], 'Term_id': [283]},\n",
       " 'corps': {'fileNames': ['article_2.tsv'], 'Term_id': [279]},\n",
       " 'heavili': {'fileNames': ['article_2.tsv'], 'Term_id': [314]},\n",
       " 'birthmark': {'fileNames': ['article_2.tsv'], 'Term_id': [268]},\n",
       " 'empti': {'fileNames': ['article_2.tsv'], 'Term_id': [255]},\n",
       " 'socket': {'fileNames': ['article_2.tsv'], 'Term_id': [258]},\n",
       " 'right': {'fileNames': ['article_2.tsv'], 'Term_id': [291]},\n",
       " 'hand': {'fileNames': ['article_2.tsv'], 'Term_id': [252]},\n",
       " 'attack': {'fileNames': ['article_2.tsv'], 'Term_id': [313]},\n",
       " 'rape': {'fileNames': ['article_2.tsv'], 'Term_id': [305]},\n",
       " 'evil': {'fileNames': ['article_2.tsv'], 'Term_id': [311]},\n",
       " 'spirit': {'fileNames': ['article_2.tsv', 'article_3.tsv'],\n",
       "  'Term_id': [312, 329]},\n",
       " 'bedroom': {'fileNames': ['article_2.tsv'], 'Term_id': [280]},\n",
       " 'later': {'fileNames': ['article_2.tsv'], 'Term_id': [295]},\n",
       " 'disturb': {'fileNames': ['article_2.tsv'], 'Term_id': [299]},\n",
       " 'encount': {'fileNames': ['article_2.tsv'], 'Term_id': [297]},\n",
       " 'sila': {'fileNames': ['article_2.tsv'], 'Term_id': [270]},\n",
       " 'woodsman': {'fileNames': ['article_2.tsv'], 'Term_id': [271]},\n",
       " 'live': {'fileNames': ['article_2.tsv'], 'Term_id': [256]},\n",
       " 'lodg': {'fileNames': ['article_2.tsv'], 'Term_id': [259]},\n",
       " 'ident': {'fileNames': ['article_2.tsv'], 'Term_id': [272]},\n",
       " 'other': {'fileNames': ['article_2.tsv'], 'Term_id': [287]},\n",
       " 'reluct': {'fileNames': ['article_2.tsv'], 'Term_id': [307]},\n",
       " 'tell': {'fileNames': ['article_2.tsv'], 'Term_id': [261]},\n",
       " 'anyth': {'fileNames': ['article_2.tsv'], 'Term_id': [302]},\n",
       " 'show': {'fileNames': ['article_2.tsv'], 'Term_id': [253]},\n",
       " 'willing': {'fileNames': ['article_2.tsv'], 'Term_id': [308]},\n",
       " 'answer': {'fileNames': ['article_2.tsv'], 'Term_id': [266]},\n",
       " 'question': {'fileNames': ['article_2.tsv'], 'Term_id': [290]},\n",
       " 'kill': {'fileNames': ['article_2.tsv', 'article_4.tsv'],\n",
       "  'Term_id': [281, 387]},\n",
       " 'bizarr': {'fileNames': ['article_2.tsv'], 'Term_id': [284]},\n",
       " 'circumst': {'fileNames': ['article_2.tsv'], 'Term_id': [260]},\n",
       " 'maitland': {'fileNames': ['article_2.tsv'], 'Term_id': [273]},\n",
       " 'solicitor': {'fileNames': ['article_2.tsv'], 'Term_id': [293]},\n",
       " 'hack': {'fileNames': ['article_2.tsv'], 'Term_id': [274]},\n",
       " 'luke': {'fileNames': ['article_2.tsv'], 'Term_id': [288]},\n",
       " 'housemaid': {'fileNames': ['article_2.tsv'], 'Term_id': [262]},\n",
       " 'thrown': {'fileNames': ['article_2.tsv'], 'Term_id': [277]},\n",
       " 'stair': {'fileNames': ['article_2.tsv'], 'Term_id': [292]},\n",
       " 'aunt': {'fileNames': ['article_2.tsv'], 'Term_id': [304]},\n",
       " 'edith': {'fileNames': ['article_2.tsv'], 'Term_id': [309]},\n",
       " 'chaperon': {'fileNames': ['article_2.tsv'], 'Term_id': [275]},\n",
       " 'strangl': {'fileNames': ['article_2.tsv'], 'Term_id': [276]},\n",
       " 'vanish': {'fileNames': ['article_2.tsv'], 'Term_id': [285]},\n",
       " 'asphyx': {'fileNames': ['article_3.tsv'], 'Term_id': [325]},\n",
       " 'dead': {'fileNames': ['article_3.tsv'], 'Term_id': [323]},\n",
       " 'newbrook': {'fileNames': ['article_3.tsv'], 'Term_id': [326]},\n",
       " 'stephen': {'fileNames': ['article_3.tsv'], 'Term_id': [330]},\n",
       " 'powel': {'fileNames': ['article_3.tsv'], 'Term_id': [321]},\n",
       " 'victorian': {'fileNames': ['article_3.tsv'], 'Term_id': [368]},\n",
       " 'england': {'fileNames': ['article_3.tsv'], 'Term_id': [346]},\n",
       " 'philanthrop': {'fileNames': ['article_3.tsv'], 'Term_id': [352]},\n",
       " 'scientist': {'fileNames': ['article_3.tsv'], 'Term_id': [363]},\n",
       " 'hugo': {'fileNames': ['article_3.tsv'], 'Term_id': [343]},\n",
       " 'cunningham': {'fileNames': ['article_3.tsv'], 'Term_id': [335]},\n",
       " 'member': {'fileNames': ['article_3.tsv'], 'Term_id': [365]},\n",
       " 'parapsycholog': {'fileNames': ['article_3.tsv'], 'Term_id': [367]},\n",
       " 'societi': {'fileNames': ['article_3.tsv'], 'Term_id': [334]},\n",
       " 'studi': {'fileNames': ['article_3.tsv'], 'Term_id': [333]},\n",
       " 'psychic': {'fileNames': ['article_3.tsv'], 'Term_id': [347]},\n",
       " 'phenomena': {'fileNames': ['article_3.tsv'], 'Term_id': [336]},\n",
       " 'part': {'fileNames': ['article_3.tsv'], 'Term_id': [332]},\n",
       " 'latest': {'fileNames': ['article_3.tsv'], 'Term_id': [348]},\n",
       " 'investig': {'fileNames': ['article_3.tsv'], 'Term_id': [360]},\n",
       " 'begun': {'fileNames': ['article_3.tsv'], 'Term_id': [366]},\n",
       " 'photograph': {'fileNames': ['article_3.tsv'], 'Term_id': [357]},\n",
       " 'individu': {'fileNames': ['article_3.tsv'], 'Term_id': [350]},\n",
       " 'moment': {'fileNames': ['article_3.tsv'], 'Term_id': [340]},\n",
       " 'done': {'fileNames': ['article_3.tsv'], 'Term_id': [361]},\n",
       " 'properli': {'fileNames': ['article_3.tsv'], 'Term_id': [337]},\n",
       " 'photo': {'fileNames': ['article_3.tsv'], 'Term_id': [339]},\n",
       " 'depict': {'fileNames': ['article_3.tsv'], 'Term_id': [362]},\n",
       " 'strang': {'fileNames': ['article_3.tsv'], 'Term_id': [338]},\n",
       " 'smudg': {'fileNames': ['article_3.tsv'], 'Term_id': [356]},\n",
       " 'hover': {'fileNames': ['article_3.tsv'], 'Term_id': [359]},\n",
       " 'around': {'fileNames': ['article_3.tsv'], 'Term_id': [354]},\n",
       " 'bodi': {'fileNames': ['article_3.tsv'], 'Term_id': [353]},\n",
       " 'though': {'fileNames': ['article_3.tsv'], 'Term_id': [345]},\n",
       " 'conclud': {'fileNames': ['article_3.tsv'], 'Term_id': [341]},\n",
       " 'captur': {'fileNames': ['article_3.tsv'], 'Term_id': [351]},\n",
       " 'evid': {'fileNames': ['article_3.tsv'], 'Term_id': [342]},\n",
       " 'soul': {'fileNames': ['article_3.tsv'], 'Term_id': [355]},\n",
       " 'escap': {'fileNames': ['article_3.tsv'], 'Term_id': [358]},\n",
       " 'sceptic': {'fileNames': ['article_3.tsv'], 'Term_id': [349]},\n",
       " 'assassin': {'fileNames': ['article_4.tsv'], 'Term_id': [381]},\n",
       " 'thriller': {'fileNames': ['article_4.tsv'], 'Term_id': [378]},\n",
       " 'crane': {'fileNames': ['article_4.tsv'], 'Term_id': [376]},\n",
       " 'hendri': {'fileNames': ['article_4.tsv'], 'Term_id': [374]},\n",
       " 'edward': {'fileNames': ['article_4.tsv'], 'Term_id': [371]},\n",
       " 'judd': {'fileNames': ['article_4.tsv'], 'Term_id': [380]},\n",
       " 'frank': {'fileNames': ['article_4.tsv'], 'Term_id': [377]},\n",
       " 'windsor': {'fileNames': ['article_4.tsv'], 'Term_id': [369]},\n",
       " 'hire': {'fileNames': ['article_4.tsv'], 'Term_id': [386]},\n",
       " 'ministri': {'fileNames': ['article_4.tsv'], 'Term_id': [384]},\n",
       " 'defenc': {'fileNames': ['article_4.tsv'], 'Term_id': [389]},\n",
       " 'offici': {'fileNames': ['article_4.tsv'], 'Term_id': [388]},\n",
       " 'suspect': {'fileNames': ['article_4.tsv'], 'Term_id': [391]},\n",
       " 'leak': {'fileNames': ['article_4.tsv'], 'Term_id': [385]},\n",
       " 'secret': {'fileNames': ['article_4.tsv'], 'Term_id': [390]}}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = 'C:\\\\Users\\\\Profosor\\\\Desktop\\\\tsv'\n",
    "createDictionary(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bomb\n",
      "1 execut\n",
      "2 direct\n",
      "3 michael\n",
      "4 growth\n",
      "5 entir\n",
      "6 world\n",
      "7 featur\n",
      "8 dystopian\n",
      "9 whose\n",
      "10 concern\n",
      "11 sell\n",
      "12 book\n",
      "13 design\n",
      "14 bleak\n",
      "15 scienc\n",
      "16 1972\n",
      "17 chaplin\n",
      "18 popul\n",
      "19 fiction\n",
      "20 year\n",
      "21 violat\n",
      "22 oliv\n",
      "23 best\n",
      "24 american\n",
      "25 earth\n",
      "26 bound\n",
      "27 danish\n",
      "28 paul\n",
      "29 denmark\n",
      "30 oppress\n",
      "31 reflect\n",
      "32 futur\n",
      "33 overpopul\n",
      "34 govern\n",
      "35 children\n",
      "36 film\n",
      "37 campu\n",
      "38 inspir\n",
      "39 ehrlich\n",
      "40 almost\n",
      "41 zero\n",
      "42 star\n",
      "43 geraldin\n",
      "44 reed\n",
      "45 short\n",
      "46 perman\n",
      "47 babyland\n",
      "48 surfac\n",
      "49 entir\n",
      "50 world\n",
      "51 visit\n",
      "52 penalti\n",
      "53 animatron\n",
      "54 sever\n",
      "55 bear\n",
      "56 common\n",
      "57 resourc\n",
      "58 past\n",
      "59 result\n",
      "60 color\n",
      "61 deterr\n",
      "62 plastic\n",
      "63 brainwash\n",
      "64 dome\n",
      "65 mask\n",
      "66 coupl\n",
      "67 need\n",
      "68 pollut\n",
      "69 conceiv\n",
      "70 substitut\n",
      "71 popul\n",
      "72 breath\n",
      "73 yearn\n",
      "74 extinct\n",
      "75 newborn\n",
      "76 bright\n",
      "77 smog\n",
      "78 next\n",
      "79 given\n",
      "80 thick\n",
      "81 citi\n",
      "82 outsid\n",
      "83 reduc\n",
      "84 suffoc\n",
      "85 well\n",
      "86 earth\n",
      "87 ultim\n",
      "88 avail\n",
      "89 affect\n",
      "90 anim\n",
      "91 household\n",
      "92 cover\n",
      "93 wear\n",
      "94 futur\n",
      "95 overpopul\n",
      "96 becom\n",
      "97 govern\n",
      "98 children\n",
      "99 life\n",
      "100 death\n",
      "101 size\n",
      "102 decre\n",
      "103 break\n",
      "104 robot\n",
      "105 contain\n",
      "106 place\n",
      "107 parent\n",
      "108 dismal\n",
      "109 settl\n",
      "110 year\n",
      "111 even\n",
      "112 child\n",
      "113 instead\n",
      "114 peopl\n",
      "115 tasteless\n",
      "116 compani\n",
      "117 york\n",
      "118 direct\n",
      "119 hutton\n",
      "120 shot\n",
      "121 shepperton\n",
      "122 seven\n",
      "123 drama\n",
      "124 michael\n",
      "125 base\n",
      "126 last\n",
      "127 woman\n",
      "128 mullin\n",
      "129 known\n",
      "130 famili\n",
      "131 separ\n",
      "132 taylor\n",
      "133 london\n",
      "134 citat\n",
      "135 edna\n",
      "136 screenplay\n",
      "137 whose\n",
      "138 concern\n",
      "139 album\n",
      "140 cain\n",
      "141 pictur\n",
      "142 design\n",
      "143 studio\n",
      "144 1972\n",
      "145 singl\n",
      "146 bicker\n",
      "147 upon\n",
      "148 coupl\n",
      "149 need\n",
      "150 locat\n",
      "151 song\n",
      "152 brien\n",
      "153 fool\n",
      "154 british\n",
      "155 well\n",
      "156 marriag\n",
      "157 middl\n",
      "158 novel\n",
      "159 three\n",
      "160 elizabeth\n",
      "161 cover\n",
      "162 film\n",
      "163 circl\n",
      "164 come\n",
      "165 releas\n",
      "166 columbia\n",
      "167 night\n",
      "168 brian\n",
      "169 star\n",
      "170 side\n",
      "171 susannah\n",
      "172 theme\n",
      "173 director\n",
      "174 york\n",
      "175 rock\n",
      "176 drawn\n",
      "177 quiet\n",
      "178 michael\n",
      "179 architect\n",
      "180 loud\n",
      "181 term\n",
      "182 match\n",
      "183 taylor\n",
      "184 whose\n",
      "185 owner\n",
      "186 cain\n",
      "187 person\n",
      "188 blake\n",
      "189 socialit\n",
      "190 coars\n",
      "191 frequent\n",
      "192 complet\n",
      "193 antithesi\n",
      "194 husband\n",
      "195 spar\n",
      "196 antic\n",
      "197 marriag\n",
      "198 sick\n",
      "199 stella\n",
      "200 elizabeth\n",
      "201 boutiqu\n",
      "202 robert\n",
      "203 someth\n",
      "204 susannah\n",
      "205 verbal\n",
      "206 compani\n",
      "207 antholog\n",
      "208 direct\n",
      "209 fengriffen\n",
      "210 length\n",
      "211 portmanteau\n",
      "212 base\n",
      "213 known\n",
      "214 featur\n",
      "215 ogilvi\n",
      "216 start\n",
      "217 screenplay\n",
      "218 scream\n",
      "219 case\n",
      "220 bride\n",
      "221 roger\n",
      "222 bray\n",
      "223 1970\n",
      "224 ward\n",
      "225 beacham\n",
      "226 four\n",
      "227 mage\n",
      "228 cush\n",
      "229 novella\n",
      "230 hous\n",
      "231 court\n",
      "232 patrick\n",
      "233 david\n",
      "234 british\n",
      "235 best\n",
      "236 gothic\n",
      "237 baker\n",
      "238 villag\n",
      "239 amicu\n",
      "240 horror\n",
      "241 herbert\n",
      "242 written\n",
      "243 film\n",
      "244 larg\n",
      "245 1973\n",
      "246 stephani\n",
      "247 near\n",
      "248 oakley\n",
      "249 marshal\n",
      "250 star\n",
      "251 stori\n",
      "252 hand\n",
      "253 show\n",
      "254 fengriffen\n",
      "255 empti\n",
      "256 live\n",
      "257 fiancé\n",
      "258 socket\n",
      "259 lodg\n",
      "260 circumst\n",
      "261 tell\n",
      "262 housemaid\n",
      "263 famili\n",
      "264 estat\n",
      "265 vision\n",
      "266 answer\n",
      "267 ogilvi\n",
      "268 birthmark\n",
      "269 sever\n",
      "270 sila\n",
      "271 woodsman\n",
      "272 ident\n",
      "273 maitland\n",
      "274 hack\n",
      "275 chaperon\n",
      "276 strangl\n",
      "277 thrown\n",
      "278 beacham\n",
      "279 corps\n",
      "280 bedroom\n",
      "281 kill\n",
      "282 move\n",
      "283 undead\n",
      "284 bizarr\n",
      "285 vanish\n",
      "286 experi\n",
      "287 other\n",
      "288 luke\n",
      "289 hous\n",
      "290 question\n",
      "291 right\n",
      "292 stair\n",
      "293 solicitor\n",
      "294 terrifi\n",
      "295 later\n",
      "296 charl\n",
      "297 encount\n",
      "298 catherin\n",
      "299 disturb\n",
      "300 begin\n",
      "301 film\n",
      "302 anyth\n",
      "303 death\n",
      "304 aunt\n",
      "305 rape\n",
      "306 night\n",
      "307 reluct\n",
      "308 willing\n",
      "309 edith\n",
      "310 1795\n",
      "311 evil\n",
      "312 spirit\n",
      "313 attack\n",
      "314 heavili\n",
      "315 british\n",
      "316 fiction\n",
      "317 film\n",
      "318 direct\n",
      "319 death\n",
      "320 robert\n",
      "321 powel\n",
      "322 scienc\n",
      "323 dead\n",
      "324 1972\n",
      "325 asphyx\n",
      "326 newbrook\n",
      "327 known\n",
      "328 star\n",
      "329 spirit\n",
      "330 stephen\n",
      "331 horror\n",
      "332 part\n",
      "333 studi\n",
      "334 societi\n",
      "335 cunningham\n",
      "336 phenomena\n",
      "337 properli\n",
      "338 strang\n",
      "339 photo\n",
      "340 moment\n",
      "341 conclud\n",
      "342 evid\n",
      "343 hugo\n",
      "344 result\n",
      "345 though\n",
      "346 england\n",
      "347 psychic\n",
      "348 latest\n",
      "349 sceptic\n",
      "350 individu\n",
      "351 captur\n",
      "352 philanthrop\n",
      "353 bodi\n",
      "354 around\n",
      "355 soul\n",
      "356 smudg\n",
      "357 photograph\n",
      "358 escap\n",
      "359 hover\n",
      "360 investig\n",
      "361 done\n",
      "362 depict\n",
      "363 scientist\n",
      "364 death\n",
      "365 member\n",
      "366 begun\n",
      "367 parapsycholog\n",
      "368 victorian\n",
      "369 windsor\n",
      "370 british\n",
      "371 edward\n",
      "372 direct\n",
      "373 film\n",
      "374 hendri\n",
      "375 1973\n",
      "376 crane\n",
      "377 frank\n",
      "378 thriller\n",
      "379 star\n",
      "380 judd\n",
      "381 assassin\n",
      "382 british\n",
      "383 govern\n",
      "384 ministri\n",
      "385 leak\n",
      "386 hire\n",
      "387 kill\n",
      "388 offici\n",
      "389 defenc\n",
      "390 secret\n",
      "391 suspect\n",
      "392 assassin\n"
     ]
    }
   ],
   "source": [
    "for l,t in enumerate(vocabulary):\n",
    "    print(l,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('C://Users//Profosor//Desktop//tsv/index-file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(321, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term_id</th>\n",
       "      <th>word</th>\n",
       "      <th>fileNames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "      <td>short</td>\n",
       "      <td>article_0.tsv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>zero</td>\n",
       "      <td>article_0.tsv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>popul</td>\n",
       "      <td>article_0.tsv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>growth</td>\n",
       "      <td>article_0.tsv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>1972</td>\n",
       "      <td>article_0.tsv, article_1.tsv, article_3.tsv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Term_id    word                                    fileNames\n",
       "0       45   short                                article_0.tsv\n",
       "1       41    zero                                article_0.tsv\n",
       "2       18   popul                                article_0.tsv\n",
       "3        4  growth                                article_0.tsv\n",
       "4       16    1972  article_0.tsv, article_1.tsv, article_3.tsv"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term_id</th>\n",
       "      <th>word</th>\n",
       "      <th>fileNames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>390</td>\n",
       "      <td>secret</td>\n",
       "      <td>article_4.tsv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Term_id    word      fileNames\n",
       "320      390  secret  article_4.tsv"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['word']=='secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{45: ['article_0.tsv'],\n",
       " 41: ['article_0.tsv'],\n",
       " 18: ['article_0.tsv'],\n",
       " 4: ['article_0.tsv'],\n",
       " 16: ['article_0.tsv', ' article_1.tsv', ' article_3.tsv'],\n",
       " 27: ['article_0.tsv'],\n",
       " 24: ['article_0.tsv'],\n",
       " 8: ['article_0.tsv'],\n",
       " 15: ['article_0.tsv', ' article_3.tsv'],\n",
       " 19: ['article_0.tsv', ' article_3.tsv'],\n",
       " 36: ['article_0.tsv',\n",
       "  ' article_1.tsv',\n",
       "  ' article_2.tsv',\n",
       "  ' article_3.tsv',\n",
       "  ' article_4.tsv'],\n",
       " 2: ['article_0.tsv',\n",
       "  ' article_1.tsv',\n",
       "  ' article_2.tsv',\n",
       "  ' article_3.tsv',\n",
       "  ' article_4.tsv'],\n",
       " 3: ['article_0.tsv', ' article_1.tsv'],\n",
       " 37: ['article_0.tsv'],\n",
       " 42: ['article_0.tsv',\n",
       "  ' article_1.tsv',\n",
       "  ' article_2.tsv',\n",
       "  ' article_3.tsv',\n",
       "  ' article_4.tsv'],\n",
       " 22: ['article_0.tsv'],\n",
       " 44: ['article_0.tsv'],\n",
       " 43: ['article_0.tsv'],\n",
       " 17: ['article_0.tsv'],\n",
       " 38: ['article_0.tsv'],\n",
       " 23: ['article_0.tsv', ' article_2.tsv'],\n",
       " 11: ['article_0.tsv'],\n",
       " 12: ['article_0.tsv'],\n",
       " 0: ['article_0.tsv'],\n",
       " 28: ['article_0.tsv'],\n",
       " 39: ['article_0.tsv'],\n",
       " 10: ['article_0.tsv', ' article_1.tsv'],\n",
       " 33: ['article_0.tsv'],\n",
       " 32: ['article_0.tsv'],\n",
       " 25: ['article_0.tsv'],\n",
       " 9: ['article_0.tsv', ' article_1.tsv'],\n",
       " 6: ['article_0.tsv'],\n",
       " 34: ['article_0.tsv', ' article_4.tsv'],\n",
       " 1: ['article_0.tsv'],\n",
       " 21: ['article_0.tsv'],\n",
       " 20: ['article_0.tsv'],\n",
       " 35: ['article_0.tsv'],\n",
       " 29: ['article_0.tsv'],\n",
       " 40: ['article_0.tsv'],\n",
       " 5: ['article_0.tsv'],\n",
       " 26: ['article_0.tsv'],\n",
       " 7: ['article_0.tsv', ' article_2.tsv'],\n",
       " 13: ['article_0.tsv', ' article_1.tsv'],\n",
       " 31: ['article_0.tsv'],\n",
       " 14: ['article_0.tsv'],\n",
       " 30: ['article_0.tsv'],\n",
       " 96: ['article_0.tsv'],\n",
       " 54: ['article_0.tsv', ' article_2.tsv'],\n",
       " 68: ['article_0.tsv'],\n",
       " 114: ['article_0.tsv'],\n",
       " 67: ['article_0.tsv', ' article_1.tsv'],\n",
       " 93: ['article_0.tsv'],\n",
       " 72: ['article_0.tsv'],\n",
       " 65: ['article_0.tsv'],\n",
       " 82: ['article_0.tsv'],\n",
       " 89: ['article_0.tsv'],\n",
       " 88: ['article_0.tsv'],\n",
       " 57: ['article_0.tsv'],\n",
       " 46: ['article_0.tsv'],\n",
       " 80: ['article_0.tsv'],\n",
       " 77: ['article_0.tsv'],\n",
       " 109: ['article_0.tsv'],\n",
       " 108: ['article_0.tsv'],\n",
       " 81: ['article_0.tsv'],\n",
       " 92: ['article_0.tsv', ' article_1.tsv'],\n",
       " 48: ['article_0.tsv'],\n",
       " 90: ['article_0.tsv'],\n",
       " 111: ['article_0.tsv'],\n",
       " 56: ['article_0.tsv'],\n",
       " 91: ['article_0.tsv'],\n",
       " 74: ['article_0.tsv'],\n",
       " 115: ['article_0.tsv'],\n",
       " 76: ['article_0.tsv'],\n",
       " 60: ['article_0.tsv'],\n",
       " 58: ['article_0.tsv'],\n",
       " 62: ['article_0.tsv'],\n",
       " 105: ['article_0.tsv'],\n",
       " 83: ['article_0.tsv'],\n",
       " 102: ['article_0.tsv'],\n",
       " 69: ['article_0.tsv'],\n",
       " 78: ['article_0.tsv'],\n",
       " 103: ['article_0.tsv'],\n",
       " 59: ['article_0.tsv', ' article_3.tsv'],\n",
       " 100: ['article_0.tsv', ' article_2.tsv', ' article_3.tsv'],\n",
       " 52: ['article_0.tsv'],\n",
       " 107: ['article_0.tsv'],\n",
       " 85: ['article_0.tsv', ' article_1.tsv'],\n",
       " 75: ['article_0.tsv'],\n",
       " 63: ['article_0.tsv'],\n",
       " 104: ['article_0.tsv'],\n",
       " 70: ['article_0.tsv'],\n",
       " 73: ['article_0.tsv'],\n",
       " 87: ['article_0.tsv'],\n",
       " 61: ['article_0.tsv'],\n",
       " 106: ['article_0.tsv'],\n",
       " 64: ['article_0.tsv'],\n",
       " 84: ['article_0.tsv'],\n",
       " 66: ['article_0.tsv', ' article_1.tsv'],\n",
       " 112: ['article_0.tsv'],\n",
       " 55: ['article_0.tsv'],\n",
       " 51: ['article_0.tsv'],\n",
       " 47: ['article_0.tsv'],\n",
       " 79: ['article_0.tsv'],\n",
       " 99: ['article_0.tsv'],\n",
       " 101: ['article_0.tsv'],\n",
       " 53: ['article_0.tsv'],\n",
       " 113: ['article_0.tsv'],\n",
       " 129: ['article_1.tsv', ' article_2.tsv', ' article_3.tsv'],\n",
       " 116: ['article_1.tsv', ' article_2.tsv'],\n",
       " 154: ['article_1.tsv', ' article_2.tsv', ' article_3.tsv', ' article_4.tsv'],\n",
       " 123: ['article_1.tsv'],\n",
       " 168: ['article_1.tsv'],\n",
       " 119: ['article_1.tsv'],\n",
       " 160: ['article_1.tsv'],\n",
       " 132: ['article_1.tsv'],\n",
       " 140: ['article_1.tsv'],\n",
       " 171: ['article_1.tsv'],\n",
       " 117: ['article_1.tsv'],\n",
       " 165: ['article_1.tsv'],\n",
       " 166: ['article_1.tsv'],\n",
       " 141: ['article_1.tsv'],\n",
       " 125: ['article_1.tsv', ' article_2.tsv'],\n",
       " 147: ['article_1.tsv'],\n",
       " 158: ['article_1.tsv'],\n",
       " 135: ['article_1.tsv'],\n",
       " 152: ['article_1.tsv'],\n",
       " 136: ['article_1.tsv', ' article_2.tsv'],\n",
       " 157: ['article_1.tsv'],\n",
       " 146: ['article_1.tsv'],\n",
       " 156: ['article_1.tsv'],\n",
       " 126: ['article_1.tsv'],\n",
       " 127: ['article_1.tsv'],\n",
       " 164: ['article_1.tsv'],\n",
       " 120: ['article_1.tsv'],\n",
       " 121: ['article_1.tsv'],\n",
       " 143: ['article_1.tsv'],\n",
       " 150: ['article_1.tsv'],\n",
       " 133: ['article_1.tsv'],\n",
       " 173: ['article_1.tsv'],\n",
       " 128: ['article_1.tsv'],\n",
       " 172: ['article_1.tsv'],\n",
       " 151: ['article_1.tsv'],\n",
       " 163: ['article_1.tsv'],\n",
       " 159: ['article_1.tsv'],\n",
       " 167: ['article_1.tsv', ' article_2.tsv'],\n",
       " 139: ['article_1.tsv'],\n",
       " 122: ['article_1.tsv'],\n",
       " 131: ['article_1.tsv'],\n",
       " 153: ['article_1.tsv'],\n",
       " 170: ['article_1.tsv'],\n",
       " 145: ['article_1.tsv'],\n",
       " 130: ['article_1.tsv', ' article_2.tsv'],\n",
       " 134: ['article_1.tsv'],\n",
       " 188: ['article_1.tsv'],\n",
       " 180: ['article_1.tsv'],\n",
       " 190: ['article_1.tsv'],\n",
       " 203: ['article_1.tsv'],\n",
       " 189: ['article_1.tsv'],\n",
       " 179: ['article_1.tsv'],\n",
       " 194: ['article_1.tsv'],\n",
       " 202: ['article_1.tsv', ' article_3.tsv'],\n",
       " 175: ['article_1.tsv'],\n",
       " 191: ['article_1.tsv'],\n",
       " 205: ['article_1.tsv'],\n",
       " 195: ['article_1.tsv'],\n",
       " 182: ['article_1.tsv'],\n",
       " 198: ['article_1.tsv'],\n",
       " 196: ['article_1.tsv'],\n",
       " 176: ['article_1.tsv'],\n",
       " 177: ['article_1.tsv'],\n",
       " 201: ['article_1.tsv'],\n",
       " 185: ['article_1.tsv'],\n",
       " 199: ['article_1.tsv'],\n",
       " 192: ['article_1.tsv'],\n",
       " 193: ['article_1.tsv'],\n",
       " 181: ['article_1.tsv'],\n",
       " 187: ['article_1.tsv'],\n",
       " 218: ['article_2.tsv'],\n",
       " 216: ['article_2.tsv'],\n",
       " 245: ['article_2.tsv', ' article_4.tsv'],\n",
       " 236: ['article_2.tsv'],\n",
       " 240: ['article_2.tsv', ' article_3.tsv'],\n",
       " 224: ['article_2.tsv'],\n",
       " 237: ['article_2.tsv'],\n",
       " 228: ['article_2.tsv'],\n",
       " 241: ['article_2.tsv'],\n",
       " 232: ['article_2.tsv'],\n",
       " 227: ['article_2.tsv'],\n",
       " 246: ['article_2.tsv'],\n",
       " 225: ['article_2.tsv'],\n",
       " 215: ['article_2.tsv'],\n",
       " 210: ['article_2.tsv'],\n",
       " 251: ['article_2.tsv'],\n",
       " 239: ['article_2.tsv'],\n",
       " 207: ['article_2.tsv'],\n",
       " 211: ['article_2.tsv'],\n",
       " 242: ['article_2.tsv'],\n",
       " 221: ['article_2.tsv'],\n",
       " 249: ['article_2.tsv'],\n",
       " 223: ['article_2.tsv'],\n",
       " 229: ['article_2.tsv'],\n",
       " 209: ['article_2.tsv'],\n",
       " 233: ['article_2.tsv'],\n",
       " 219: ['article_2.tsv'],\n",
       " 220: ['article_2.tsv'],\n",
       " 244: ['article_2.tsv'],\n",
       " 230: ['article_2.tsv'],\n",
       " 248: ['article_2.tsv'],\n",
       " 231: ['article_2.tsv'],\n",
       " 247: ['article_2.tsv'],\n",
       " 222: ['article_2.tsv'],\n",
       " 238: ['article_2.tsv'],\n",
       " 226: ['article_2.tsv'],\n",
       " 300: ['article_2.tsv'],\n",
       " 310: ['article_2.tsv'],\n",
       " 282: ['article_2.tsv'],\n",
       " 296: ['article_2.tsv'],\n",
       " 264: ['article_2.tsv'],\n",
       " 298: ['article_2.tsv'],\n",
       " 286: ['article_2.tsv'],\n",
       " 294: ['article_2.tsv'],\n",
       " 265: ['article_2.tsv'],\n",
       " 283: ['article_2.tsv'],\n",
       " 279: ['article_2.tsv'],\n",
       " 314: ['article_2.tsv'],\n",
       " 268: ['article_2.tsv'],\n",
       " 255: ['article_2.tsv'],\n",
       " 258: ['article_2.tsv'],\n",
       " 291: ['article_2.tsv'],\n",
       " 252: ['article_2.tsv'],\n",
       " 313: ['article_2.tsv'],\n",
       " 305: ['article_2.tsv'],\n",
       " 311: ['article_2.tsv'],\n",
       " 312: ['article_2.tsv', ' article_3.tsv'],\n",
       " 280: ['article_2.tsv'],\n",
       " 295: ['article_2.tsv'],\n",
       " 299: ['article_2.tsv'],\n",
       " 297: ['article_2.tsv'],\n",
       " 270: ['article_2.tsv'],\n",
       " 271: ['article_2.tsv'],\n",
       " 256: ['article_2.tsv'],\n",
       " 259: ['article_2.tsv'],\n",
       " 272: ['article_2.tsv'],\n",
       " 287: ['article_2.tsv'],\n",
       " 307: ['article_2.tsv'],\n",
       " 261: ['article_2.tsv'],\n",
       " 302: ['article_2.tsv'],\n",
       " 253: ['article_2.tsv'],\n",
       " 308: ['article_2.tsv'],\n",
       " 266: ['article_2.tsv'],\n",
       " 290: ['article_2.tsv'],\n",
       " 281: ['article_2.tsv', ' article_4.tsv'],\n",
       " 284: ['article_2.tsv'],\n",
       " 260: ['article_2.tsv'],\n",
       " 273: ['article_2.tsv'],\n",
       " 293: ['article_2.tsv'],\n",
       " 274: ['article_2.tsv'],\n",
       " 288: ['article_2.tsv'],\n",
       " 262: ['article_2.tsv'],\n",
       " 277: ['article_2.tsv'],\n",
       " 292: ['article_2.tsv'],\n",
       " 304: ['article_2.tsv'],\n",
       " 309: ['article_2.tsv'],\n",
       " 275: ['article_2.tsv'],\n",
       " 276: ['article_2.tsv'],\n",
       " 285: ['article_2.tsv'],\n",
       " 325: ['article_3.tsv'],\n",
       " 323: ['article_3.tsv'],\n",
       " 326: ['article_3.tsv'],\n",
       " 330: ['article_3.tsv'],\n",
       " 321: ['article_3.tsv'],\n",
       " 368: ['article_3.tsv'],\n",
       " 346: ['article_3.tsv'],\n",
       " 352: ['article_3.tsv'],\n",
       " 363: ['article_3.tsv'],\n",
       " 343: ['article_3.tsv'],\n",
       " 335: ['article_3.tsv'],\n",
       " 365: ['article_3.tsv'],\n",
       " 367: ['article_3.tsv'],\n",
       " 334: ['article_3.tsv'],\n",
       " 333: ['article_3.tsv'],\n",
       " 347: ['article_3.tsv'],\n",
       " 336: ['article_3.tsv'],\n",
       " 332: ['article_3.tsv'],\n",
       " 348: ['article_3.tsv'],\n",
       " 360: ['article_3.tsv'],\n",
       " 366: ['article_3.tsv'],\n",
       " 357: ['article_3.tsv'],\n",
       " 350: ['article_3.tsv'],\n",
       " 340: ['article_3.tsv'],\n",
       " 361: ['article_3.tsv'],\n",
       " 337: ['article_3.tsv'],\n",
       " 339: ['article_3.tsv'],\n",
       " 362: ['article_3.tsv'],\n",
       " 338: ['article_3.tsv'],\n",
       " 356: ['article_3.tsv'],\n",
       " 359: ['article_3.tsv'],\n",
       " 354: ['article_3.tsv'],\n",
       " 353: ['article_3.tsv'],\n",
       " 345: ['article_3.tsv'],\n",
       " 341: ['article_3.tsv'],\n",
       " 351: ['article_3.tsv'],\n",
       " 342: ['article_3.tsv'],\n",
       " 355: ['article_3.tsv'],\n",
       " 358: ['article_3.tsv'],\n",
       " 349: ['article_3.tsv'],\n",
       " 381: ['article_4.tsv'],\n",
       " 378: ['article_4.tsv'],\n",
       " 376: ['article_4.tsv'],\n",
       " 374: ['article_4.tsv'],\n",
       " 371: ['article_4.tsv'],\n",
       " 380: ['article_4.tsv'],\n",
       " 377: ['article_4.tsv'],\n",
       " 369: ['article_4.tsv'],\n",
       " 386: ['article_4.tsv'],\n",
       " 384: ['article_4.tsv'],\n",
       " 389: ['article_4.tsv'],\n",
       " 388: ['article_4.tsv'],\n",
       " 391: ['article_4.tsv'],\n",
       " 385: ['article_4.tsv'],\n",
       " 390: ['article_4.tsv']}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.copy()\n",
    "# df2.drop('word',axis=0,inplace=True)\n",
    "t_id = df2['Term_id']\n",
    "f_name = [x.split(',') for x in df2['fileNames']]\n",
    "# l=[x for x in df2['Term_id']]\n",
    "# for i in f_name:\n",
    "#     new = [x.strip() for x in i]\n",
    "#     l.append(new)\n",
    "    \n",
    "d= dict(zip(t_id,f_name))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Profosor\\\\Desktop\\\\tsv\\\\d.json', 'w') as f:\n",
    "    json.dump(d, f,indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Profosor\\\\Desktop\\\\tsv\\\\d.json', 'r') as f:\n",
    "    data =  json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Z.P.G.</td>\n",
       "      <td>short zero popul growth 1972 danish american d...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Z.P.G.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zee and Co.</td>\n",
       "      <td>known compani 1972 british drama film direct b...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Zee_and_Co.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And Now the Screaming Starts!</td>\n",
       "      <td>scream start 1973 british gothic horror film d...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/And_Now_the_Scre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           title  \\\n",
       "0                         Z.P.G.   \n",
       "1                    Zee and Co.   \n",
       "2  And Now the Screaming Starts!   \n",
       "\n",
       "                                               intro  \\\n",
       "0  short zero popul growth 1972 danish american d...   \n",
       "1  known compani 1972 british drama film direct b...   \n",
       "2  scream start 1973 british gothic horror film d...   \n",
       "\n",
       "                                                 url  \n",
       "0               https://en.wikipedia.org/wiki/Z.P.G.  \n",
       "1          https://en.wikipedia.org/wiki/Zee_and_Co.  \n",
       "2  https://en.wikipedia.org/wiki/And_Now_the_Scre...  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = ['secret','popul','whose']\n",
    "# query = [x.lower() for x in query]\n",
    "\n",
    "title = []\n",
    "intro = []\n",
    "url = []\n",
    "for i,term in enumerate(list(set(vocabulary))):\n",
    "    os.chdir('C:/Users/Profosor/Desktop/tsv')\n",
    "    for q in query:\n",
    "        if q == term:\n",
    "            file_list = d[i]\n",
    "            for j in file_list:\n",
    "                li=[]\n",
    "                with open(j,'r') as f:\n",
    "                    data = f.read().strip().split('\\n')\n",
    "                    for i in data: \n",
    "                        if i!= '':\n",
    "                            li.append(i)\n",
    "                title.append(li[0].strip())\n",
    "                intro.append(li[1])\n",
    "                url.append(li[3])\n",
    "\n",
    "                        \n",
    "dict = {'title':title,'intro':intro,'url':url}\n",
    "\n",
    "df = pd.DataFrame(dict) \n",
    "\n",
    "    \n",
    "df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
